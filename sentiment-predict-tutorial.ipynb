{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8faa0994-9760-4a48-b71b-9f218b963cea",
   "metadata": {},
   "source": [
    "# Data Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "98bf12a8-9c72-4a34-9996-984d99bd89b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "class Sentiment:\n",
    "    NEGATIVE = \"NEGATIVE\"\n",
    "    NEUTRAL = \"NEUTRAL\"\n",
    "    POSITIVE = \"POSITIVE\"\n",
    "\n",
    "class Review:\n",
    "    def __init__(self, text, score):\n",
    "        self.text = text\n",
    "        self.score = score\n",
    "        self.sentiment = self.get_sentiment()\n",
    "\n",
    "    def get_sentiment(self):\n",
    "        if self.score <= 2:\n",
    "            return Sentiment.NEGATIVE\n",
    "        elif self.score == 3:\n",
    "            return Sentiment.NEUTRAL\n",
    "        else: #Score of 4 or 5\n",
    "            return Sentiment.POSITIVE\n",
    "        \n",
    "        \n",
    "class ReviewContainer:\n",
    "    def __init__(self, reviews):\n",
    "        self.reviews = reviews\n",
    "    \n",
    "    def get_text(self):\n",
    "        return [x.text for x in self.reviews]\n",
    "    \n",
    "    def get_sentiment(self):\n",
    "        return [x.sentiment for x in self.reviews]\n",
    "        \n",
    "    def evenly_distribute(self):\n",
    "        negative = list(filter(lambda x: x.sentiment == Sentiment.NEGATIVE, self.reviews))\n",
    "        positive = list(filter(lambda x: x.sentiment == Sentiment.POSITIVE, self.reviews))\n",
    "        positive_shrunk = positive[:len(negative)]\n",
    "        self.reviews = negative + positive_shrunk\n",
    "        random.shuffle(self.reviews)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8aa3814c-2749-42f1-adf9-f5cf624db8e5",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f63cf3fd-8d16-4088-82fa-0593ae65cc31",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'I hoped for Mia to have some peace in this book, but her story is so real and raw.  Broken World was so touching and emotional because you go from Mia\\'s trauma to her trying to cope.  I love the way the story displays how there is no \"just bouncing back\" from being sexually assaulted.  Mia showed us how those demons come for you every day and how sometimes they best you. I was so in the moment with Broken World and hurt with Mia because she was surrounded by people but so alone and I understood her feelings.  I found myself wishing I could give her some of my courage and strength or even just to be there for her.  Thank you Lizzy for putting a great character\\'s voice on a strong subject and making it so that other peoples story may be heard through Mia\\'s.'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "file_name = './books_small_10000.json'\n",
    "\n",
    "reviews = []\n",
    "with open(file_name) as f:\n",
    "    for line in f:\n",
    "        review = json.loads(line)\n",
    "        reviews.append(Review(review['reviewText'], review['overall']))\n",
    "\n",
    "reviews[5].text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e484c473-546a-4fdd-9706-b331f50b9782",
   "metadata": {},
   "source": [
    "# Prep Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0b17ec5d-c160-4ff2-9ed4-3ef0fc6f5891",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "training, test = train_test_split(reviews, test_size=0.33, random_state=42)\n",
    "\n",
    "train_container = ReviewContainer(training)\n",
    "test_container = ReviewContainer(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "15651e60-4dc7-4713-a099-f034df04ea88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "436\n",
      "436\n",
      "208\n",
      "208\n"
     ]
    }
   ],
   "source": [
    "# Evenly distribute the training data\n",
    "train_container.evenly_distribute()\n",
    "test_container.evenly_distribute()\n",
    "\n",
    "X_train = train_container.get_text()\n",
    "y_train = train_container.get_sentiment()\n",
    "\n",
    "X_test = test_container.get_text()\n",
    "y_test = test_container.get_sentiment()\n",
    "\n",
    "print(y_train.count(Sentiment.NEGATIVE))\n",
    "print(y_train.count(Sentiment.POSITIVE))\n",
    "print(y_test.count(Sentiment.NEGATIVE))\n",
    "print(y_test.count(Sentiment.POSITIVE))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6f27231-4217-429a-a404-f427b7523884",
   "metadata": {},
   "source": [
    "# Bags of Words Vectorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b7c9f176-9aeb-402f-9cad-4500923e87a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "\n",
    "\n",
    "# The problem with the CountVectorizer is that it weights all words equally.\n",
    "# vectorizer = CountVectorizer()\n",
    "\n",
    "# Instead, let's use the TfidfVectorizer which weights words based on their importance\n",
    "vectorizer = TfidfVectorizer()\n",
    "\n",
    "# fit and transform training data\n",
    "X_train_vectors = vectorizer.fit_transform(X_train)\n",
    "\n",
    "# transform test data\n",
    "X_test_vectors = vectorizer.transform(X_test)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c18af34b",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8e3bad03",
   "metadata": {},
   "source": [
    "# Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c05ed78c",
   "metadata": {},
   "source": [
    "### Linear SVM (Support Vector Machine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ff086bf4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I liked this book. Fast, easy read. I liked catching up about the first couple in this series. Great history about Louisiana. Love a happy ending!!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array(['POSITIVE'], dtype='<U8')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn import svm\n",
    "\n",
    "clf_svm = svm.SVC(kernel='linear')\n",
    "\n",
    "clf_svm.fit(X_train_vectors, y_train)\n",
    "\n",
    "print(X_test[0])\n",
    "clf_svm.predict(X_test_vectors[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39bea78e",
   "metadata": {},
   "source": [
    "### Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ba451dc2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['POSITIVE'], dtype='<U8')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "clf_dec = DecisionTreeClassifier()\n",
    "\n",
    "clf_dec.fit(X_train_vectors, y_train)\n",
    "\n",
    "clf_dec.predict(X_test_vectors[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "418ddea0",
   "metadata": {},
   "source": [
    "### Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1f1e1fa0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['POSITIVE'], dtype='<U8')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "clf_gnb = GaussianNB()\n",
    "\n",
    "clf_gnb.fit(X_train_vectors.toarray(), y_train)\n",
    "\n",
    "clf_gnb.predict(X_test_vectors[0].toarray())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65bd72a3",
   "metadata": {},
   "source": [
    "### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0fe312e4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['POSITIVE'], dtype='<U8')"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "clf_log = LogisticRegression()\n",
    "\n",
    "clf_log.fit(X_train_vectors, y_train)\n",
    "\n",
    "clf_log.predict(X_test_vectors[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2376cdc5",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c2163669",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8076923076923077\n",
      "0.6442307692307693\n",
      "0.6610576923076923\n",
      "0.8028846153846154\n"
     ]
    }
   ],
   "source": [
    "# MEAN ACCURACY\n",
    "# Comparison of test vectors and test data\n",
    "# .score() method returns the mean accuracy on the given test data and labels.\n",
    "print(clf_svm.score(X_test_vectors, y_test))\n",
    "print(clf_dec.score(X_test_vectors, y_test))\n",
    "print(clf_gnb.score(X_test_vectors.toarray(), y_test))\n",
    "print(clf_log.score(X_test_vectors, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c106845a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.80582524, 0.80952381])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# F1 SCORE\n",
    "# The F1 score can be interpreted as a weighted average of the precision and recall,\n",
    "# where an F1 score reaches its best value at 1 and worst score at 0.\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "f1_score(y_test, clf_svm.predict(X_test_vectors), average=None, labels=[Sentiment.POSITIVE, Sentiment.NEGATIVE])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a3410319",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['POSITIVE', 'NEGATIVE', 'NEGATIVE'], dtype='<U8')"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_set = ['great for the price', 'horrible, never buy', 'not bad']\n",
    "new_test = vectorizer.transform(test_set)\n",
    "\n",
    "clf_svm.predict(new_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b21ca4f5",
   "metadata": {},
   "source": [
    "## Tuning our model (with Grid Search)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "25c8b18f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Parameters: {'C': 4, 'kernel': 'rbf'}\n",
      "Best Score: 0.8371559934318556\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "parameters = {'kernel': ('linear', 'rbf'), 'C': (1,4,8,16,32)}\n",
    "\n",
    "svc = svm.SVC()\n",
    "clf = GridSearchCV(svc, parameters, cv=5)\n",
    "clf.fit(X_train_vectors, y_train)\n",
    "\n",
    "print(\"Best Parameters:\", clf.best_params_)\n",
    "print(\"Best Score:\", clf.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d692b88c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8197115384615384\n"
     ]
    }
   ],
   "source": [
    "print(clf.score(X_test_vectors, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9da0d11",
   "metadata": {},
   "source": [
    "## Saving Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44e88971",
   "metadata": {},
   "source": [
    "### Save Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "63b7b680-0d18-4301-835e-641d6b85d46e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#save the model so that we can use it later\n",
    "import pickle\n",
    "\n",
    "# 'wb' means write in binary, which is the format that the pickle module uses to write to files.\n",
    "# pickle.dump() method writes the pickled representation of the object to the open file.\n",
    "with open('./models/sentiment_classifier.pkl', 'wb') as f:\n",
    "    # taking our classifier object and saving it to the file\n",
    "    pickle.dump(clf, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5685bdf1",
   "metadata": {},
   "source": [
    "### Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "7054dc8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./models/sentiment_classifier.pkl', 'rb') as f:\n",
    "    loaded_clf = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57cac1f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_clf.predict(X_test_vectors[0])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
